The EM algorithm is used to extract the domain model Domθ that maximizes P(Dom| θ"Dom) (where Dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = DomD Dt Dtc CDom DomDom tPtP DomP Dom Dom ; ' ||1maxarg |maxarg θηθη θθ θ θ (6) This is the same process as the one used to extract feedback model in [35].
The score depending on the domain model is then as follows: ∑∈ = Vt D Dom QDom tPtPDQScore )|(log)|(),( θθ (8) Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.
Therefore, we further construct a sub domain model more related to the given query, by using a subset of in domain documents that are related to the query.
We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.
This model is combined with both baseline models (with or without feedback).
Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2. 30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6. Domain models with top 100 documents (C2) Domain Sub Domain Coll.
Third, without Feedback model, the sub domain models constructed with relevant documents perform much better than the whole domain models (Tab. 5).
We have integrated the above contextual factors, together with feedback model, in a single language model.
