However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per term smoothing, while in a multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily accommodating per term smoothing and weighting.
We find that while with some smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing.
Experiment results show that the Poisson model with perterm smoothing outperforms multinomial model, and the performance can be further improved with two stage smoothing.
The parameter sensitivity curves for two Jelinek Mercer 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 0.3 Dataset: Trec8 Parameter: δ AveragePrecision JM−Multinomial: LV JM−Multinomial: SV JM−Multinomial: SK JM−Poisson: SK JM−Poisson: SV JM−Poisson: LV Figure 1: Poisson and multinomial performs similarly with Jelinek Mercer smoothing smoothing methods are shown in Figure 1.
Given that this type of mixture 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.05 0.1 0.15 0.2 0.25 Data: Trec8; Query: SV Parameter: δ AveragePrecision Poisson Background K−Mixture Background Figure 3: K Mixture background model deviates the sensitivity of verbose queries background model does not require any extra computation cost, it would be interesting to study whether using other mixture Poisson models, such as 2 Poisson and negative Binomial, could help the performance.
Document language models, query models, and risk minimization for information retrieval.
J. Lafferty and C. Zhai. Probabilistic IR models based on query and document generation.
In W. B. Croft and J. Lafferty, editors, Language Modeling and Information Retrieval.
