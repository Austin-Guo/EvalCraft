Utility based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu  H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation.
Tracking new and relevant information from temporal data streams for users with long lasting needs has been a challenging research topic in information retrieval.
Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.
The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage.
This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms   and thus   usually the same information.
It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.
Our experiments on the newly annotated TDT4 benchmark corpus show encouraging utility enhancement over Indri, and also over our own system with incremental learning and novelty detection turned off.
We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.
