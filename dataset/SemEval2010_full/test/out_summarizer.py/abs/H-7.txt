Recent offerings such as My MSN, My Yahoo!, My Google, and Google News have attracted much attention due to their potential ability to infer a user"s interests from his/her history.
A user profile is usually a classifier that can identify whether a document is relevant to the user or not, or a regression model that tells how relevant a document is to the user.
The basic idea is that instead of calculating the numerical solution for all the user profile parameters, we derive the analytical solution of the parameters for some feature dimensions, and at the M step use the analytical solution instead of the numerical solution estimated at E step for those parameters.
The experimental setting and results used to validate the proposed learning technique are reported in Sections 5 and 6.
In the rest of the paper, we use these symbols to represent their diagonal approximations: Σ2 =     σ2 1 0 .. 0 0 σ2 2 .. 0 .. .. .. .. 0 0 .. σ2 K     Σ2 m =     σ2 m,1 0 .. 0 0 σ2 m,2 .. 0 .. .. .. .. 0 0 .. σ2 m,K     Secondly, and most importantly, the input space is very sparse and there are many dimensions that are not related to a particular user in a real IR application.
For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters E Reuters C, ReutersM and Reuters G. For each small data set, we created several profiles, one profile for each node in a sub tree, to simulate multiple users, each with a related, yet separate definition of relevance.
We first evaluated the performance on each individual user, and then estimated the macro average over all users.
Norm 2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models.
