A good document clustering approach can assist the computers to automatically organize the document corpus into a meaningful cluster hierarchy for efficient browsing and navigation, which is very valuable for complementing the deficiencies of traditional information retrieval technologies.
The hierarchical methods group the data points into a hierarchical tree structure using bottom up or top down approaches.
As we know that there are two main problems existing in partitioning methods (like Kmeans and Gaussian Mixture Model (GMM) [16]): (1) the predefined criterion is usually non convex which causes many local optimal solutions; (2) the iterative procedure (e.g. the Expectation Maximization (EM) algorithm) for optimizing the criterions usually makes the final solutions heavily depend on the initializations.
Generally speaking, learning can be posed as a problem of function estimation, from which we can get a good classification function that will assign labels to the training dataset and even the unseen testing dataset with some cost minimized [24].
There are mainly two approaches to achieve this goal: 1. As in [20], we can treat the i th row of Q as the embedding of xi in a C dimensional space, and apply some traditional clustering methods like kmeans to clustering these embeddings into C clusters.
In this method we just minimize Jl (defined in Eq.(24)), and the clustering results can be obtained by doing eigenvalue decomposition on matrix (I − P)T (I − P) with some proper discretization methods.
The final discretization method adopted in these two methods is the same as in [26], since our experiments show that using such method can achieve better results than using kmeans based methods as in [20].
The clustering accuracies comparison results are shown in table 3, and the normalized mutual information comparison results are summarized in table 4.
