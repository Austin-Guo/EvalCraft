Some recorded examples were: • MI1: Looking for obituaries and other material on the novelist John Fowles, who died at the weekend.
For example, consider the following recorded task: • LU4: re find AS"s paper on graded relevance assessments because I want to see how she presented her results for a paper I am writing This task actually consists of two sub tasks: 1 item task(refind the paper) and 1 lookup task (look for specific information within the paper).
For example: • MI4: re find Kelkoo website so that I can re check the prices of hair straighteners for my girlfriend A second source of ambiguity came from tasks such as finding an email containing a URL as a means of re accessing a web page.
Information that had been seen less than a day or less than a week before the task were defined as hot, information that had been seen less than a month before the task as warm, and information that had been seen less than a year or more than a year before the task as cold.
For the look up task data, there was evidence that hot tasks were perceived easier than cold (p=0.0001) and that warm tasks were perceived easier than cold tasks(p=0.0041), but there was no evidence to distinguish between the difficulty ratings of hot and warm tasks(p=0.593).
There is also the additional benefit that the task descriptions would not make any assumptions about what the user would remember in a real life situation because they would only include the information that had been recorded i.e. the information that was available when the user originally performed the task.
Of the 30 tasks issued, 26 (86.67%) were completed without problems, 2 (6.67%) of the tasks were not completed because the description recorded was insufficent to recreate the task, and 2 tasks (6.67%) were not completed because the task was too difficult or the required web page no longer existed.
We would like to thank Dr Mark Baillie for his insightful comments and help analysing the data.
