We provide a framework for the analysis of the trade off between static caching of query answers and posting lists; â€¢ Static caching of terms can be more effective than dynamic caching with, for example, LRU.
Buckley and Lewit [3], in one of the earliest works, take a term at a time approach to deciding when inverted lists need not be further examined.
Based on the observations of Markatos, Lempel and Moran propose a new caching policy, called Probabilistic Driven Caching, by attempting to estimate the probability distribution of all possible queries submitted to a search engine [8].
It is possible that queries that are most frequent after the cache have different characteristics, and tuning the search engine to queries frequent before the cache may degrade performance for non cached queries.
The first strategy we consider is the algorithm proposed by Baeza Yates and Saint Jean [2], which consists in selecting the posting lists of the terms with the highest query term frequencies fq(t).
In fact, the problem of selecting the best posting lists for the static cache corresponds to the standard Knapsack problem: given a knapsack of fixed capacity, and a set of n items, such as the i th item has value ci and size si, select the set of items that fit in the knapsack and maximize the overall value.
In this case, the total amount of memory is split between the broker, which holds the cached 400 500 600 700 800 900 1000 1100 1200 0 0.2 0.4 0.6 0.8 1 Averageresponsetime Space (GB) Simulated workload    single machine full / uncompr / 1 G partial / uncompr / 1 G full / compr / 0.5 G partial / compr / 0.5 G Figure 11: Optimal division of the cache in a server.
To support this claim, we first assess how topics change comparing the distribution of queries from the first week in June, 2006, to the distribution of queries for the remainder of 2006 that did not appear in the first week in June.
