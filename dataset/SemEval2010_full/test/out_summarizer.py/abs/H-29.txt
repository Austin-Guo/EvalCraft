Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the user"s information need may be vague or incompletely specified by these queries.
In theory, the scoring function can vary from query to query, although in this study for simplicity we keep the scoring function the same for all queries.
We assume that the multinomial feedback models {ˆθ1, . . . , ˆθB} were generated by a latent Dirichlet distribution with parameters {α1, . . . , αN }. To estimate the {α1, . . . , αN }, we fit the Dirichlet parameters to the B language model samples according to maximum likelihood using a generalized Newton procedure, details of which are given in Minka [13].
This method first selects terms using a log odds calculation described by Ponte [14], but assigns final term weights using Lavrenko"s relevance model[10].
Queries for which initial average precision was negligible (≤ 0.01) were ignored, giving the remaining query count in column N. of −1.0, when all queries are hurt by the feedback method, to +1.0 when all queries are helped.
On the document side, recent work by Zhou & Croft [21] explored the idea of adding noise to documents, re scoring them, and using the stability of the resulting rankings as an estimate of query difficulty.
Ando and Zhang proposed a method that they call structural feedback [3] and showed how to apply it to query expansion for the TREC Genomics Track.
We thank Paul Bennett for valuable discussions related to this work, which was supported by NSF grants #IIS 0534345 and #CNS 0454018, and U.S. Dept. of Education grant #R305G03123.
