Utility based Information Distillation Over Temporally Sequenced Documents Yiming Yang Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA yiming@cs.cmu.edu Abhimanyu Lad Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA alad@cs.cmu.edu Ni Lao Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA nlao@cs.cmu.edu Abhay Harpale Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA aharpale@cs.cmu.edu Bryan Kisiel Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA bkisiel@cs.cmu.edu Monica Rogati Language Technologies Inst. Carnegie Mellon University Pittsburgh, USA mrogati@cs.cmu.edu  H.3.3 [Information Search and Retrieval]: Information filtering, Relevance feedback, Retrieval models, Selection process; I.5.2 General Terms Design, Measurement, Performance, Experimentation.
We have conducted our experiments on this extended TDT4 corpus and have made the additionally generated data publicly available for future comparative evaluations 1 . To automatically evaluate the system returned arbitrary spans of text using our answer keys, we further developed an evaluation scheme with semi automatic procedure for 1 URL: http://nyc.lti.cs.cmu.edu/downloads acquiring rules that can match nuggets against system responses.
Thirdly, since the output of the system is ranked lists, we must reward those systems that present useful information (both relevant and previously unseen) using shorter ranked lists, and penalize those that present the same information using longer ranked lists.
However, without the loss term, DCG is a purely recall oriented metric and not suitable for an adaptive filtering setting, where the system"s utility depends in part on its ability to limit the number of items shown to the user.
The DCU score obtained by the system is converted to a Normalized DCU (NDCU) score by dividing it by the DCU score of the ideal ranked list, which is created by ordering passages by their decreasing utility scores U(pi, q) and stopping when U(pi, q) ≤ 0 i.e. when the gain is less than or equal to the cost of reading the passage.
This reveals a shortcoming of contemporary retrieval systemswhen the user gives positive feedback on a passage, the systems gives higher weights to the terms present in that passage and tends to retrieve other passages containing the same terms   and thus   usually the same information.
It is informative to evaluate retrieval systems using our utility measure (with γ = 0) which accounts for novelty and thus gives a more realistic picture of how well a system can generalize from user feedback, rather than using traditional IR measures like recall and precision which give an incomplete picture of improvement obtained from user feedback.
We would like to thank Rosta Farzan, Jonathan Grady, Jaewook Ahn, Yefei Peng, and the Qualitative Data Analysis Program at the University of Pittsburgh lead by Dr. Stuart Shulman for their help with collecting and processing the extended TDT4 annotations used in our experiments.
