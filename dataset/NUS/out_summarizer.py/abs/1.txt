Subtleties arise since the success of this mechanism
depends on restricting the given sources, and so recursion
constantly changes the original sources.
Beyond the obvious motivations (potential
use of physical sources in pseudorandom generators and in
derandomization), extractors have found applications in a
671
variety of areas in theoretical computer science where randomness
does not seem an issue, such as in efficient constructions
of communication networks [24, 7], error correcting
codes [22, 12], data structures [14] and more.
This bound was matched by Alon [1] using the Polynomial
Method, by Grolmusz [11] using low rank matrices over rings,
and also by Barak [2] boosting Abbot's method with almost
k wise independent random variables (a construction that
was independently discovered by others as well).
To simplify the presentation, in this version of the paper
we will assume that we are working with entropy as opposed
to min entropy.
The techniques of [4] involved partitioning
the sources into t pieces of length n/t each, with the hope
that one of those parts would have a significant amount of
entropy, yet there'd be enough entropy left over in the rest
of the source (so that the source can be partitioned into a
block source).
THE FINAL DISPERSER
D
Following is a rough description of our disperser D proving
Theorem 2.1.
For example, in the construction of our final disperser
, to ensure that our algorithms correctly identify the
right high block to recurse on, we were only able to guarantee
that there are subsources of the original sources in
which our algorithm makes the correct decision with high
probability.
The advantage of this definition
is that it says that once we restrict our attention to
the good subsources X
good
, Y
good
, we have the freedom to further
restrict these subsources to smaller subsources, as long
as our final subsources do not lose more entropy than the
resiliency permits.
