Categories and Subject Descriptors
I.5.2 [PATTERN RECOGNITION]: Design Methodology
  Classifier design and evaluation; H.2.8 [Database Management
]: Database Applications  Data mining
General Terms
Design, Algorithms, Experimentation
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page.
Belkin et al. developed a framework
for regularization on graphs and provided some analysis
on generalization error bounds [1].
The goal of supervised learning
is to find a prediction function p(X) that minimizes the
following expected true loss:
E
(X,Y )D
L(p(X), Y ),
where E
(X,Y )D
denotes the expectation over the true underlying
distribution
D. In order to achieve a stable esimia tion
, we usually need to restrict the size of hypothesis function
family.
The authors in [27] show a theoretical justification
that designing a kernel matrix with faster spectral decay
rates should result in better generalization performance,
which offers an important pricinple in learning an effective
kernel matrix.
For a comparison, we denote this method as
K
Trunc
.
191
Research Track Paper
0
5
10
15
20
25
30
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Dimension (d)
Scaled Coefficient
Original Kernel
SKL (C=1)
SKL (C=2)
SKL (C=3)
(a) Spectral coefficients
0
10
20
30
40
50
0.7
0.75
0.8
0.85
0.9
0.95
1
Dimension (d)
Accuracy
K
Origin
K
Trunc
K
Cluster
K
Spectral
(b) C=1
0
10
20
30
40
50
0.7
0.75
0.8
0.85
0.9
0.95
1
Dimension (d)
Accuracy
K
Origin
K
Trunc
K
Cluster
K
Spectral
(c) C=2
Figure 4:
Example of Spectral coefficients and performance impacted by different decay factors on the
Ionosphere dataset.
Each cell in the table has two rows: the upper
row shows the average testing set accruacies with standard
errors; and the lower row gives the average run time in seconds
for learning the semi supervised kernels on a 3GHz
desktop computer.
For the proposed framework, we focus our attention
on tackling a core problem of learning semi supervised kernels
from labeled and unlabled data.
The upper row shows the test set accuracy with standard
error; the lower row gives the average time used in learning the semi supervised kernels ("Order" and "Imp Order"
kernels are sovled by SeDuMi/YALMIP package; "SKL" kernels are solved directly by the Matlab
quadprog function.
