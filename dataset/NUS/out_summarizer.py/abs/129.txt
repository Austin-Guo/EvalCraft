Instead
of making predictions from a single model fit on the observed data, bootstrap samples
are taken of the data, the model is fit on each sample, and the predictions are averaged
over all of the fitted models to get the bagged prediction.
He also gives a theoretical explanation of how
bagging works, demonstrating the reduction in mean squared prediction error for unstable
c 2004 Herbert K. H. Lee and Merlise A. Clyde.
Such algorithms are also advantageous for extremely large data sets where reading through
the data just once is time consuming, so batch algorithms which would require multiple
passes through the data would be infeasible.
As the sample size grows large, this factor becomes
arbitrarily close to one, but we do note that it is strictly less than one, so the Bayesian
approach does give a further reduction in variance compared to the standard approach.
Furthermore, since
batch Bayesian bagging gives the same mean results as ordinary batch bagging, online
Bayesian bagging also has the same expected results as ordinary batch bagging.
We note that for
many models, such as classification trees, this renormalization is not a major issue; for a
tree, each split only depends on the relative weights of the observations at that node, so
nodes not involving the new observation will have the same ratio of weights before and
after renormalization and the rest of the tree structure will be unaffected; in practice, in
most implementations of trees (including that used in this paper), renormalization is not
necessary.
We compared Bayesian bagging to
a single tree, ordinary batch bagging, and ordinary online bagging, all three of which were
done using the minimum description length criterion (MDL), as implemented in the ITI
code, to determine the optimal size for each tree.
For each data set, we repeated 1000 times the following procedure
: randomly choose a training/test partition; fit a single tree, a batch bagged tree, an
online bagged tree, and a Bayesian bagged tree; compute the misclassification error rate for
each fit.
