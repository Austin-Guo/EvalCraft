In fact, Web
sites are not really browsed breadth first and various
restrictions may apply, e.g. limiting crawling processes
to within a site, or downloading the pages deemed
most interesting first
2
  Repetitive Crawling: once pages have been crawled,
some systems require the process to be repeated
periodically so that indexes are kept updated.
However,
some tasks are common to all scenarios, such as
2
See [9] for the heuristics that tend to find the most
important pages first and [17] for experimental results
proving that breadth first crawling allows the swift retrieval
of pages with a high PageRank.
Lawrence and Giles [16] carried out two experiments
in order to measure coverage performance of data
established by crawlers and of their updates.
Lawrence and Giles used a set of controlled data of
575 requests to provide page samples and count the number
of times that the two crawlers retrieved the same pages.
This
approach has shown that the Web contained at least 320
million pages in 1997 and that only 60% was covered by the
six major search engines of that time.
Disk access
is reduced to a minimum as far as possible and structures
are stored in the real time Mnesia
3
database that forms
3
http://www.erlang.org/doc/r9c/lib/mnesia 4
.1.4/doc/html/
301
a standard part of the Erlang development kit.
As the system seeks
maximum network accessibility (communication protocol
support), libCURL appeared to be the most judicious choice
when compared with other available libraries.
From these results, we
may label our system as being High Availability, as opposed
to other architectures that consider High Availability only
in the sense of failure not affecting other components of
the system, but in which service restart of a component
unfortunately requires manual intervention every time.
