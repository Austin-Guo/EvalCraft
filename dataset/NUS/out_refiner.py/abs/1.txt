To simplify the presentation, in this version of the paper
we will assume that we are working with entropy as opposed
to min entropy.
Theorem 1.9
([4]).
Again, when the input sources are block sources with sufficiently
many blocks, Rao proves that 2 independent sources
suffice (though this result does rely on arithmetic combinatorics
, in particular, on Bourgain's extractor).
Theorem 1.11
([?]).
The techniques of [4] involved partitioning
the sources into t pieces of length n/t each, with the hope
that one of those parts would have a significant amount of
entropy, yet there'd be enough entropy left over in the rest
of the source (so that the source can be partitioned into a
block source).
On the one hand we will have
to partition our sources into blocks of length significantly
more than n (or the adversary could distribute a negligible
fraction of entropy in all blocks).
Let us turn to the highlights of the analysis, for the proof
of Theorem 2.6.
The advantage of this definition
is that it says that once we restrict our attention to
the good subsources X
good
, Y
good
, we have the freedom to further
restrict these subsources to smaller subsources, as long
as our final subsources do not lose more entropy than the
resiliency permits.
