As noted by Hauptmann
[14], this splits the semantic gap between low level features
and user information needs into two, hopefully smaller gaps:
(a) mapping the low level features into the intermediate semantic
concepts and (b) mapping these concepts into user
needs.
Fig.1 illustrates
the procedure of smoothing the kernel matrix via clustering.
Take k means as
an example, due to the fact that the squared Euclidean distance
is used as the distortion measure, the clusters must be
separated by piece wise hyperplanes (i.e., voronoi diagram).
By transforming a dense graph into
a sparse graph, we only need store the sparse affinity matrix
instead of the original kernel matrix.
This
is unacceptable for large scale data.
The
current implementation of SCMs involves the following parameter
settings: (a) Gaussian kernel is adopted and the parameters
are selected via cross validation, furthermore, the
kernel function of kernel k means clustering is adopted the
same as that of SVMs, (b) the threshold for transforming
dense graphs to sparse ones is experimentally determined
as
= 0.6, (c) the parameter of shrinking technique is experimentally
chosen as  = 1.3, (d) for SCMs, the data are
imbalanced for each concept, we only carry out data clustering
for negative classes, therefore, k
+
always equals
|D
+
|
and k
is
always chosen as
|D
|
10
, (e) we stop the iteration
of SCMs when the number of the negative examples are not
more than the five times of that of the positive examples.
K. S. Goh, E. Y. Chang, and W. C. Lai. Multimodal
Concept dependent Active Learning for Image
Retrieval.
H. Yu, J. Yang, J. Han, and X. Li. Making SVMs
Scalable to Large Data Sets using Hierarchical Cluster
Indexing.
