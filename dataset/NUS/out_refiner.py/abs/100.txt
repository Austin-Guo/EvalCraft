In fact, Web
sites are not really browsed breadth first and various
restrictions may apply, e.g. limiting crawling processes
to within a site, or downloading the pages deemed
most interesting first
2
  Repetitive Crawling: once pages have been crawled,
some systems require the process to be repeated
periodically so that indexes are kept updated.
Dominos RPC Concurrent (cRPC): as its name suggests
, this process is responsible for delegating the
execution of certain remote functions to other processes
.
From these results, we
may label our system as being High Availability, as opposed
to other architectures that consider High Availability only
in the sense of failure not affecting other components of
the system, but in which service restart of a component
unfortunately requires manual intervention every time.
Activates a local
cRPC.
In Figure 3 we see that URL normalization
0
500000
1e+06
1.5e+06
2e+06
2.5e+06
3e+06
3.5e+06
0
2000
4000
6000
8000
10000
Time (microsec)
Documents
Average number of parsed documents
PD
0
500000
1e+06
1.5e+06
2e+06
2.5e+06
3e+06
3.5e+06
0
20
40
60
80
100
120
140
160
Time (microsec)
Document Size (Mb)
Average size of parsed documents
PDS
Figure 2: Link Extraction
is as efficient as extraction in terms of speed.
In Proc. of the 8th Int.
Breadth first search
crawling yields high quality pages.
S. Russel and P. Norvig.
