For mining constrained gradients in data cubes, we first consider a naive approach which computes such gradients by conducting a search, for the gradient cells, once per probe cell 1 .
We aim to design such an algorithm, with the additional bonus that, in the second step, the algorithm will not check more significant cells than All-Significant-Pairs, it will examine each significant cell only once (i.e., one pass), and it will use the probe cells to constrain the search in an "optimized" way.
C p if the following is true: (1) If the gradient constraint is anti-monotone (such as the sum constraint), then C grad (c g , c p ) is satisfied for some c p # C p . (2) If the gradient constraint is not anti-monotone, such as (avg price(c g )/avg price(c f then a transformed, weaker constraint can be potentially satisfied for some c p # C p , such as represents top-k average and k is the minimum support threshold (i.e., significance constraint).
Fortunately, one can derive an overall gradient cell constraint for set C p , C gcell (C p ), which specifies a range of measure values (such as average prices) for c g and which must be satisfied by a gradient cell c g if c g might co-occur in interesting gradient-probe pairs with any probe cell in C p .
In this subsection we describe what probe cells should be associated with a gradient cell, and how to prune the associated probe cells when the processing goes from a gradient cell to a descendant one; both will be from a dimension-matching perspective.
As shown in [BR99, HPDW01], bottom-up computation in cubes is e#cient because it allows us to use low-dimension cells to prune high dimension cells.
In principle, our model and algorithm developed for constrained gradients in data cubes should be still applicable to mining transaction-based gradients with complex measures.
This study can be considered as (1) an extension of data cube computation to mining interesting gradients, and (2) an extension of constraint-based mining toward mining constrained gradients in data cubes.
